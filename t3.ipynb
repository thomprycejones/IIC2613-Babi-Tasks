{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informe Tarea 3\n",
    "## Inteligencia Artifical 2018-2\n",
    "\n",
    "<a id='Menu'></a>\n",
    "### Menú Principal\n",
    "A continuación se describirá los pasos a seguir para desarrollar la tarea:\n",
    "+ [Introducción](#Introduction)\n",
    "+ [Desarrollo](#Development)\n",
    "    + [Metodología a utilizar](#Metodology)\n",
    "        + [Pre-Procesamiento](#Pre_Proces)\n",
    "        + [Crear Espacio de Características](#Feature_Space)\n",
    "        + [Generar Vocabularios](#Vocabulary)\n",
    "        + [Traspasar Vocabularios a notación One Hot](#OneHot)\n",
    "        + [Modelo SVM](#ModelSVM)\n",
    "            + [Preprocesar](#TCPreProcesSVM)\n",
    "            + [Entrenar Clasificadores](#TCTrainClasiffierSVM)\n",
    "            + [Medir Rendimientos](#TCCheckPerformanceSVM)\n",
    "        + [Modelo NN](#ModelNN)\n",
    "            + [Preprocesar](#TCPreProcesNN)\n",
    "            + [Entrenar Clasificadores](#TCTrainClasiffierNN)\n",
    "            + [Medir Rendimientos](#TCCheckPerformanceNN)\n",
    "\n",
    "    + [bAbI-Tasks](#bAbI-Tasks)\n",
    "        + [#1 Basic factoid QA with single supporting fact](#bAbI-1)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-1)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-1)\n",
    "            + [Análisis SVM](#bAbI-NN-A-1)\n",
    "            + [Entrenar NN](#bAbI-NN-T-1)\n",
    "            + [Resultados NN](#bAbI-NN-R-1)\n",
    "            + [Análisis NN](#bAbI-NN-A-1)\n",
    "        + [#2 Factoid QA with two supporting facts](#bAbI-2)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-2)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-2)\n",
    "            + [Análisis SVM](#bAbI-NN-A-2)\n",
    "            + [Entrenar NN](#bAbI-NN-T-2)\n",
    "            + [Resultados NN](#bAbI-NN-R-2)\n",
    "            + [Análisis NN](#bAbI-NN-A-2)\n",
    "        + [#3 Factoid QA with three supporting facts](#bAbI-3)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-3)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-3)\n",
    "            + [Análisis SVM](#bAbI-NN-A-3)\n",
    "            + [Entrenar NN](#bAbI-NN-T-3)\n",
    "            + [Resultados NN](#bAbI-NN-R-3)\n",
    "            + [Análisis NN](#bAbI-NN-A-3)\n",
    "        + [#4 Two argument relations: subject vs. object](#bAbI-4)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-4)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-4)\n",
    "            + [Análisis SVM](#bAbI-NN-A-4)\n",
    "            + [Entrenar NN](#bAbI-NN-T-4)\n",
    "            + [Resultados NN](#bAbI-NN-R-4)\n",
    "            + [Análisis NN](#bAbI-NN-A-4)\n",
    "        + [#5 Three argument relations](#bAbI-5)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-5)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-5)\n",
    "            + [Análisis SVM](#bAbI-NN-A-5)\n",
    "            + [Entrenar NN](#bAbI-NN-T-5)\n",
    "            + [Resultados NN](#bAbI-NN-R-5)\n",
    "            + [Análisis NN](#bAbI-NN-A-5)\n",
    "        + [#6 Yes/No questions](#bAbI-6)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-6)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-6)\n",
    "            + [Análisis SVM](#bAbI-NN-A-6)\n",
    "            + [Entrenar NN](#bAbI-NN-T-6)\n",
    "            + [Resultados NN](#bAbI-NN-R-6)\n",
    "            + [Análisis NN](#bAbI-NN-A-6)\n",
    "        + [#7 Counting](#bAbI-7)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-7)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-7)\n",
    "            + [Análisis SVM](#bAbI-NN-A-7)\n",
    "            + [Entrenar NN](#bAbI-NN-T-7)\n",
    "            + [Resultados NN](#bAbI-NN-R-7)\n",
    "            + [Análisis NN](#bAbI-NN-A-7)\n",
    "        + [#8 Lists/Sets](#bAbI-8)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-8)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-8)\n",
    "            + [Análisis SVM](#bAbI-NN-A-8)\n",
    "            + [Entrenar NN](#bAbI-NN-T-8)\n",
    "            + [Resultados NN](#bAbI-NN-R-8)\n",
    "            + [Análisis NN](#bAbI-NN-A-8)\n",
    "        + [#9 Simple Negation](#bAbI-9)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-9)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-9)\n",
    "            + [Análisis SVM](#bAbI-NN-A-9)\n",
    "            + [Entrenar NN](#bAbI-NN-T-9)\n",
    "            + [Resultados NN](#bAbI-NN-R-9)\n",
    "            + [Análisis NN](#bAbI-NN-A-9)\n",
    "        + [#10 Indefinite Knowledge](#bAbI-10)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-10)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-10)\n",
    "            + [Análisis SVM](#bAbI-NN-A-10)\n",
    "            + [Entrenar NN](#bAbI-NN-T-10)\n",
    "            + [Resultados NN](#bAbI-NN-R-10)\n",
    "            + [Análisis NN](#bAbI-NN-A-10)\n",
    "        + [#11 Basic coreference](#bAbI-11)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-11)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-11)\n",
    "            + [Análisis SVM](#bAbI-NN-A-11)\n",
    "            + [Entrenar NN](#bAbI-NN-T-11)\n",
    "            + [Resultados NN](#bAbI-NN-R-11)\n",
    "            + [Análisis NN](#bAbI-NN-A-11)\n",
    "        + [#12 Conjunction](#bAbI-12)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-12)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-12)\n",
    "            + [Análisis SVM](#bAbI-NN-A-12)\n",
    "            + [Entrenar NN](#bAbI-NN-T-12)\n",
    "            + [Resultados NN](#bAbI-NN-R-12)\n",
    "            + [Análisis NN](#bAbI-NN-A-12)\n",
    "        + [#13 Compound coreference](#bAbI-13)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-13)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-13)\n",
    "            + [Análisis SVM](#bAbI-NN-A-13)\n",
    "            + [Entrenar NN](#bAbI-NN-T-13)\n",
    "            + [Resultados NN](#bAbI-NN-R-13)\n",
    "            + [Análisis NN](#bAbI-NN-A-13)\n",
    "        + [#14 Time manipulation](#bAbI-14)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-14)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-14)\n",
    "            + [Análisis SVM](#bAbI-NN-A-14)\n",
    "            + [Entrenar NN](#bAbI-NN-T-14)\n",
    "            + [Resultados NN](#bAbI-NN-R-14)\n",
    "            + [Análisis NN](#bAbI-NN-A-14)\n",
    "        + [#15 Basic deduction](#bAbI-15)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-15)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-15)\n",
    "            + [Análisis SVM](#bAbI-NN-A-15)\n",
    "            + [Entrenar NN](#bAbI-NN-T-15)\n",
    "            + [Resultados NN](#bAbI-NN-R-15)\n",
    "            + [Análisis NN](#bAbI-NN-A-15)\n",
    "        + [#16 Basic induction](#bAbI-16)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-16)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-16)\n",
    "            + [Análisis SVM](#bAbI-NN-A-16)\n",
    "            + [Entrenar NN](#bAbI-NN-T-16)\n",
    "            + [Resultados NN](#bAbI-NN-R-16)\n",
    "            + [Análisis NN](#bAbI-NN-A-16)\n",
    "        + [#17 Positional reasoning](#bAbI-17)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-17)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-17)\n",
    "            + [Análisis SVM](#bAbI-NN-A-17)\n",
    "            + [Entrenar NN](#bAbI-NN-T-17)\n",
    "            + [Resultados NN](#bAbI-NN-R-17)\n",
    "            + [Análisis NN](#bAbI-NN-A-17)\n",
    "        + [#18 Reasoning about size](#bAbI-18)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-18)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-18)\n",
    "            + [Análisis SVM](#bAbI-NN-A-18)\n",
    "            + [Entrenar NN](#bAbI-NN-T-18)\n",
    "            + [Resultados NN](#bAbI-NN-R-18)\n",
    "            + [Análisis NN](#bAbI-NN-A-18)\n",
    "        + [#19 Path finding](#bAbI-19)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-19)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-19)\n",
    "            + [Análisis SVM](#bAbI-NN-A-19)\n",
    "            + [Entrenar NN](#bAbI-NN-T-19)\n",
    "            + [Resultados NN](#bAbI-NN-R-19)\n",
    "            + [Análisis NN](#bAbI-NN-A-19)\n",
    "        + [#20 Reasoning about agent's motivation](#bAbI-20)\n",
    "            + [Entrenar SVM](#bAbI-SVM-T-20)\n",
    "            + [Resultados SVM](#bAbI-SVM-R-20)\n",
    "            + [Análisis SVM](#bAbI-NN-A-20)\n",
    "            + [Entrenar NN](#bAbI-NN-T-20)\n",
    "            + [Resultados NN](#bAbI-NN-R-20)\n",
    "            + [Análisis NN](#bAbI-NN-A-20)\n",
    "+ [Conclusión](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Pre_Proces'></a>\n",
    "## Pre-procesamiento\n",
    "\n",
    "[Volver al Menú](#Menu)\n",
    "\n",
    "\n",
    "En esta etapa realizaremos el proceso de separación de la información en tuplas de la siguiente manera\n",
    "    (story, question, answer). De tal forma que nos quedará una tupla con cada una de la información relevante. Además, realizaremos un proceso de tokenización de tal forma de dejar cada una de las oraciones como una lista de strings. Eliminaremos los '.' y llevaremos a minúscula cada linea. Opcionalmente se podrá realizar un proceso de eliminar las stopwords y una aplicación de steamming de cada línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result\n",
      "\n",
      "Historias: 1000\n",
      "Preguntas: 1000\n",
      "Respuestas: 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "TRAINING_PATH = 'dataset/en/qa1_single-supporting-fact_train.txt'\n",
    "OPTIONAL = 0 # please read de optional number in separate_story_question\n",
    "\n",
    "def separate_story_question(data_path, optional = 0):\n",
    "    ''' Función que separa la información en story y question_answer\n",
    "        y la entrega en una lista de la forma (stories, questions, answers)\n",
    "        \n",
    "            data_path: path del archivo a utilizar\n",
    "            optional:  0 = Sin uso de funciones opcional\n",
    "                       1 = Usar StopWords\n",
    "                       2 = Usar Steamming\n",
    "                       3 = Usar ambas\n",
    "        return\n",
    "            tuple(story:list of list, question:list of strings)\n",
    "    '''\n",
    "    story = []\n",
    "    question = []\n",
    "    answer = []\n",
    "    aux_story = []\n",
    "\n",
    "    with open(data_path, 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            token = word_tokenize(line.lower())\n",
    "            if (optional == 1 or optional == 3):\n",
    "                token = filter_stopwords(token)\n",
    "            if (optional == 2 or optional == 3):\n",
    "                token = steamming(token)\n",
    "            if token[0] == '1' in line:\n",
    "                aux_story = []\n",
    "            if '?' not in line:\n",
    "                token = [x for x in filter(lambda a: a != '.', token)]\n",
    "                # aux_story.append((token[0], token[1:]))\n",
    "                aux_story.append(token[1:])\n",
    "            else:\n",
    "                last_word_index = token.index('?') + 1\n",
    "                # answer.append((token[last_word_index], token[last_word_index + 1]))\n",
    "                answer.append(token[last_word_index])\n",
    "                # question.append((token[0], token[1:last_word_index]))\n",
    "                question.append(token[1:last_word_index])\n",
    "                stories = aux_story.copy()\n",
    "                aux_story.append(token[1:last_word_index])\n",
    "                story.append(stories)\n",
    "    return (story, question, answer)\n",
    "\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    ''' Funcion que filtra las palabras no considerando los artículos\n",
    "        \n",
    "            words: lista de palabras\n",
    "            \n",
    "        return\n",
    "            list(palabras filtradas)\n",
    "    '''\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = [w for w in words if w not in stopWords]\n",
    "    return wordsFiltered\n",
    "\n",
    "def steamming(words):\n",
    "    ''' Función que convierte las palabras a su raiz mediante el algoritmo de steamming de Porter. \n",
    "        Utiliza por default implementaciones de contribuidores de la librería ntlk\n",
    "        \n",
    "            words: lista de palabras\n",
    "        return\n",
    "            list(raices de palabras)\n",
    "    '''\n",
    "    steammer = PorterStemmer()\n",
    "    st = steammer.stem\n",
    "    steamWords = [st(word) for word in words]\n",
    "    return steamWords\n",
    "\n",
    "\n",
    "training_result = separate_story_question(TRAINING_PATH, OPTIONAL)\n",
    "print('Training result\\n')\n",
    "print('Historias: ' + str(len(training_result[0])))\n",
    "print('Preguntas: ' + str(len(training_result[1])))\n",
    "print('Respuestas: ' + str(len(training_result[2])) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación mostraremos un ejemplo de cómo queda nuestra estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['mary', 'moved', 'to', 'the', 'bathroom'],\n",
       "  ['john', 'went', 'to', 'the', 'hallway'],\n",
       "  ['where', 'is', 'mary', '?'],\n",
       "  ['daniel', 'went', 'back', 'to', 'the', 'hallway'],\n",
       "  ['sandra', 'moved', 'to', 'the', 'garden']],\n",
       " ['where', 'is', 'daniel', '?'],\n",
       " 'hallway')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 1\n",
    "display((training_result[0][num],\n",
    "         training_result[1][num],\n",
    "         training_result[2][num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Feature_Space'></a>\n",
    "## Espacio de Características\n",
    "\n",
    "[Volver al Menú](#Menu)\n",
    "\n",
    "Ahora generamos el espacio de características. Para esto primero que todo rellenaremos las historias para que tengan la misma dimensión y así no haya distinta dimensionalidad en los vectrores del espacio de características. Luego creamos los registros de la forma que se plantea en el enunciado de la tarea (story, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_TOKEN = '/fill'\n",
    "\n",
    "def max_story_lenght(stories):\n",
    "    ''' Funcion que nos entrega el largo máximo de las historias que procesaremos\n",
    "    \n",
    "            stories: todas las historias del set\n",
    "        return\n",
    "            integer que indica el valor máximo de historias\n",
    "    '''\n",
    "    len_stories = [len(x) for x in stories]\n",
    "    el_set = set(len_stories)\n",
    "    return max(len_stories)\n",
    "\n",
    "def fill_with_fill_token(stories, FILL_TOKEN):\n",
    "    ''' Funcion que rellena las historias con un token de relleno\n",
    "    \n",
    "            stories: todas las historias del set\n",
    "            FILL_TOKEN: token de relleno para el espacio de características\n",
    "        return\n",
    "            historias rellenas con la token de relleno para tener misma dimensión en el espacio de características\n",
    "    '''\n",
    "    max_story_number = max_story_lenght(stories)\n",
    "    for story in stories:\n",
    "        for num in range(max_story_number - len(story)):\n",
    "            tupla = ([FILL_TOKEN])\n",
    "            story.append((tupla))\n",
    "    return stories\n",
    "\n",
    "def create_register(result):\n",
    "    ''' Funcion que crea los registros de la forma que esta en el enunciado de la tarea\n",
    "    \n",
    "            result: resultado del primer parseo de la información objetivo\n",
    "        return\n",
    "            registros tal cual se muestra en el enunciado, una lista de tuplas de la forma (story, question, answer)\n",
    "    '''\n",
    "    registers = []\n",
    "    for num in range(len(result[0])):\n",
    "        registers.append([result[0][num], result[1][num], result[2][num]])\n",
    "    return registers\n",
    "\n",
    "stories = [element for element in training_result[0]]\n",
    "stories = fill_with_fill_token(stories, FILL_TOKEN)\n",
    "\n",
    "training_registers = create_register(training_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Vocabulary'></a>\n",
    "## Generar Vocabularios\n",
    "\n",
    "[Volver al Menú](#Menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(registers):\n",
    "    ''' Funcion que genera dos vocabularios a partir de los registros de la forma que se especifica en el \n",
    "        enunciado de esta tarea\n",
    "        \n",
    "            registers: registros de los sets de datos\n",
    "        return \n",
    "            vocabulary_x: vocabulario de preguntas y historias\n",
    "            vocabulary_y: vocabulario de respuestas\n",
    "    '''\n",
    "    vocabulary_x = set()\n",
    "    vocabulary_y = set()\n",
    "    stories = set()\n",
    "    for register in registers:\n",
    "        for sentence in register[0]:\n",
    "            stories = stories.union({word for word in sentence})\n",
    "        questions = {word for word in register[1]}\n",
    "        vocabulary_x = vocabulary_x.union(stories)\n",
    "        vocabulary_x = vocabulary_x.union(questions)\n",
    "        vocabulary_y = vocabulary_y.union(set([register[2]]))\n",
    "    vocabulary_x = {word: num for num, word in enumerate(sorted(vocabulary_x))}\n",
    "    vocabulary_y = {word: num for num, word in enumerate(sorted(vocabulary_y))}\n",
    "    return vocabulary_x, vocabulary_y\n",
    "\n",
    "vocabulary_features, vocabulary_class = generate_vocabulary(training_registers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='OneHot'></a>\n",
    "## Traspasar Vocabularios a notación One Hot\n",
    "\n",
    "[Volver al Menú](#Menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_one_hot(registers, vocab_x, vocab_y):\n",
    "    ''' Funcion que genera la notación one hoy para los registros\n",
    "    \n",
    "            registers: registros de la forma que se presenta en el enunciado de la tarea\n",
    "            vocab_x: vocabulario de historias y preguntas\n",
    "            vocab_y: vocabulario de respuestas\n",
    "        return\n",
    "            x_one_hot_array: lista de historias y preguntas con notación one hot\n",
    "            y_one_hot_array: lista de respuestas en notación one hot\n",
    "    '''\n",
    "    x_one_hot_array = []\n",
    "    y_one_hot_array = []\n",
    "    for register in registers:\n",
    "        aux_one_hot_array = []\n",
    "        for sentence in register[0]:\n",
    "            aux_one_hot_array.append(bag_of_words(sentence_to_one_hot(sentence, vocab_x)))\n",
    "        question_one_hot_array = [bag_of_words(sentence_to_one_hot(register[1], vocab_x))]\n",
    "        aux_one_hot_array.extend(question_one_hot_array)\n",
    "        x_one_hot_array.append(aux_one_hot_array)\n",
    "        y_one_hot_array.append(sentence_to_one_hot([register[2]], vocab_y))\n",
    "    return x_one_hot_array, y_one_hot_array\n",
    "\n",
    "def bag_of_words(one_hot_words_array):\n",
    "    ''' Función que toma listas de palabras en notación one hot y las suma para obtener la oración en notacion\n",
    "        one hot\n",
    "        \n",
    "            one_hot_words_array: lista de palabras en notación one hot\n",
    "        return\n",
    "            lista de la oración en notación one hot, sumando cada uno de las palabras\n",
    "    '''\n",
    "    return np.sum(one_hot_words_array, axis=0)\n",
    "\n",
    "def sentence_to_one_hot(words, vocab):\n",
    "    ''' Funcion que toma una lista de palabras y un vocabulario y la transforma a notación one hot\n",
    "           \n",
    "            words: lista de palabras\n",
    "            vocab: vocabulario de la lista de palabras\n",
    "        return\n",
    "            lista de palabras en notación one hot\n",
    "    '''\n",
    "    one_hot_words = []\n",
    "    for word in words:\n",
    "        one_hot_words.append(word_to_one_hot(word, vocab))\n",
    "    return one_hot_words\n",
    "\n",
    "def word_to_one_hot(word, vocab):\n",
    "    ''' Funcion que toma una palabra y un vocabulario y la transforma a notación one hot\n",
    "    \n",
    "            word: palabra\n",
    "            vocab: vocabulario de la palabra\n",
    "        return\n",
    "            palabra en notación one hot.\n",
    "    '''\n",
    "    max_vocab = len(vocab)\n",
    "    one_hot = np.zeros(max_vocab)\n",
    "    one_hot[vocab[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "x_one_hot, y_one_hot = create_one_hot(training_registers, vocabulary_features, vocabulary_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TrainClassifier'></a>\n",
    "## Entrenar Clasificador\n",
    "\n",
    "[Volver al Menú](#Menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TCPreProces'></a>\n",
    "### Preprocesar los datos  \n",
    "\n",
    "[Volver al Menú](#Menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TCTrainClasiffier'></a>\n",
    "### Entrenar Clasificador\n",
    "\n",
    "[Volver al Menú](#Menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TCCheckPerformance'></a>\n",
    "### Medir Rendimiento\n",
    "\n",
    "[Volver al Menú](#Menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}